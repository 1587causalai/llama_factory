# DPO 基线训练开发日志

## 项目概述

这个项目旨在实现一个DPO (Direct Preference Optimization) 训练的基线版本，通过拆解训练过程为forward、loss计算和backward阶段，深入了解DPO的工作原理。这为未来开发自己的偏好优化算法（如FooDPO）奠定基础。

## 日志

### 第一天：项目初始化与DPO原理学习

- 创建了`dpo_baseline`目录结构
- 学习了DPO算法的基本原理：
  - DPO是一种直接从人类偏好数据中优化语言模型的方法
  - 不同于RLHF需要训练额外的奖励模型，DPO直接从偏好数据中计算隐式奖励
  - 基本数学公式：最大化偏好数据中"被选中回答"的概率，同时最小化"被拒绝回答"的概率

### 待完成任务

1. **分析现有DPO实现**：
   - 研究LLaMA-Factory中现有的DPO实现
   - 了解数据格式和处理流程

2. **拆解DPO训练过程**：
   - 实现独立的forward过程，观察模型在偏好和拒绝回答上的输出分布
   - 分离loss计算逻辑，验证DPO loss的计算是否符合预期
   - 跟踪backward过程中的梯度流动

3. **基线测试**：
   - 在小模型（如Qwen1.5-0.5B）上验证DPO训练效果
   - 记录各阶段的指标变化

4. **为FooDPO做准备**：
   - 基于DPO基线，思考FooDPO的改进方向
   - 准备FooDPO的实验设计和评估方法 