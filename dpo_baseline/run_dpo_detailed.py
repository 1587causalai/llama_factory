#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
LLaMA-Factory DPOè®­ç»ƒè¯¦ç»†å®ç°è„šæœ¬ (Detailed DPO Training Implementation)
=====================================================================

æ­¤è„šæœ¬æä¾›äº†ä¸LLaMA-Factoryå‘½ä»¤è¡Œå·¥å…·ç›¸åŒçš„DPOè®­ç»ƒåŠŸèƒ½ï¼Œä½†é‡‡ç”¨äº†æ›´ç›´è§‚çš„æµç¨‹åˆ’åˆ†å’Œä¸°å¯Œçš„è¾“å‡ºä¿¡æ¯ã€‚
å®ƒæ˜¯ä¸€ä¸ªç‹¬ç«‹å¯æ‰§è¡Œçš„Pythonè„šæœ¬ï¼Œæ— éœ€ä¾èµ–å‘½ä»¤è¡Œæ¥å£å³å¯å®Œæˆå®Œæ•´çš„DPOè®­ç»ƒæµç¨‹ã€‚

è®¾è®¡ç›®çš„:
--------
1. å®ç”¨æ€§å®ç° - æä¾›ä¸€ä¸ªå¯ç«‹å³ä½¿ç”¨çš„DPOè®­ç»ƒè„šæœ¬ï¼ŒåŠŸèƒ½ä¸CLIå‘½ä»¤ç­‰æ•ˆ
2. æµç¨‹å¯è§†åŒ– - é€šè¿‡åˆ†æ®µæ‰“å°ä¿¡æ¯ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´åŠ é€æ˜å¯è§
3. ä¾¿äºé›†æˆå’Œæ‰©å±• - é€‚åˆé›†æˆåˆ°å…¶ä»–é¡¹ç›®æˆ–è¿›è¡Œè‡ªå®šä¹‰ä¿®æ”¹

ä¸»è¦åŠŸèƒ½æ¨¡å—:
-----------
- setup_logging: é…ç½®æ—¥å¿—ç³»ç»Ÿ
- print_section: æ‰“å°æ ¼å¼åŒ–çš„æ®µè½æ ‡é¢˜
- load_config_file: åŠ è½½å’Œæ˜¾ç¤ºé…ç½®æ–‡ä»¶å†…å®¹
- process_args: å¤„ç†é…ç½®æ–‡ä»¶å¹¶è¿”å›å‚æ•°å¯¹è±¡
- prepare_tokenizer_and_model: å‡†å¤‡tokenizerã€æ¨¡æ¿å’Œæ¨¡å‹
- prepare_dataset: å‡†å¤‡æ•°æ®é›†å’Œæ•°æ®æ•´ç†å™¨
- setup_trainer: è®¾ç½®DPOè®­ç»ƒå™¨
- run_dpo_training: æ‰§è¡ŒDPOè®­ç»ƒå’Œè¯„ä¼°
- run_dpo_workflow: ç»„ç»‡å®Œæ•´çš„DPOè®­ç»ƒå·¥ä½œæµç¨‹

ä¸å…¶ä»–è„šæœ¬çš„åŒºåˆ«:
--------------
- ç›¸æ¯”dpo_detailed.py: æœ¬è„šæœ¬æ›´ä¾§é‡äºå®é™…ä½¿ç”¨ï¼Œå‡å°‘äº†è°ƒè¯•ä»£ç ï¼Œå¢åŠ äº†æµç¨‹çš„æ¸…æ™°åº¦
- ç›¸æ¯”run_dpo_training.py: æœ¬è„šæœ¬æä¾›äº†æ›´è¯¦ç»†çš„è®­ç»ƒè¿‡ç¨‹è¾“å‡ºï¼Œé€‚åˆå­¦ä¹ å’Œç›‘æ§è®­ç»ƒè¿‡ç¨‹

ä½¿ç”¨åœºæ™¯:
--------
- éœ€è¦è¯¦ç»†äº†è§£è®­ç»ƒè¿‡ç¨‹æ—¶ä½¿ç”¨
- éœ€è¦ç›‘æ§è®­ç»ƒä¸­å„ä¸ªé˜¶æ®µæ€§èƒ½å’ŒçŠ¶æ€
- ä½œä¸ºå¼€å‘æ–°åŠŸèƒ½çš„åŸºç¡€æ¡†æ¶

ä½¿ç”¨æ–¹æ³•:
--------
python run_dpo_detailed.py [é…ç½®æ–‡ä»¶è·¯å¾„]

å¦‚æœä¸æä¾›é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ 'examples/train_lora/qwen1_5_0_5b_lora_dpo_test.yaml'
"""

import os
import sys
import logging
import yaml
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple

# å¯¼å…¥LLaMA-Factoryçš„å…³é”®ç»„ä»¶
from llamafactory.hparams import (
    ModelArguments,
    DataArguments,
    TrainingArguments, 
    FinetuningArguments,
    GeneratingArguments,
    get_train_args,
    read_args
)
from llamafactory.model import load_model, load_tokenizer
from llamafactory.data import (
    get_dataset, 
    get_template_and_fix_tokenizer, 
    PairwiseDataCollatorWithPadding
)
from llamafactory.train.dpo.trainer import CustomDPOTrainer
from llamafactory.train.trainer_utils import create_ref_model
from llamafactory.train.callbacks import LogCallback, ReporterCallback, PissaConvertCallback
from llamafactory.extras.constants import IGNORE_INDEX
from llamafactory.extras.logging import get_logger
from llamafactory.extras.misc import calculate_tps
from llamafactory.extras.ploting import plot_loss

# è®¾ç½®æ—¥å¿—
logger = get_logger(__name__)

def setup_logging(level=logging.INFO):
    """
    è®¾ç½®æ—¥å¿—é…ç½®
    """
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        level=level,
        handlers=[logging.StreamHandler(sys.stdout)]
    )

def print_section(title):
    """æ‰“å°å¸¦åˆ†éš”çº¿çš„æ®µè½æ ‡é¢˜"""
    print(f"\n{'=' * 50}")
    print(f" {title}")
    print(f"{'=' * 50}\n")

def load_config_file(config_path: str) -> Dict[str, Any]:
    """
    åŠ è½½YAMLé…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    print_section(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    # è¯»å–YAMLæ–‡ä»¶å†…å®¹
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # æ‰“å°é…ç½®å†…å®¹
    print("é…ç½®å†…å®¹é¢„è§ˆ:")
    for section, values in config.items():
        print(f"\n[{section}]")
        if isinstance(values, dict):
            for k, v in values.items():
                print(f"  {k}: {v}")
        else:
            print(f"  {values}")
    
    return config

def process_args(config_path: str) -> Tuple[ModelArguments, DataArguments, TrainingArguments, FinetuningArguments, GeneratingArguments]:
    """
    å¤„ç†é…ç½®æ–‡ä»¶å¹¶è¿”å›å‚æ•°å¯¹è±¡
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        è§£æåçš„å‚æ•°å¯¹è±¡å…ƒç»„
    """
    # ä¿å­˜å¹¶ä¿®æ”¹å‘½ä»¤è¡Œå‚æ•°ï¼Œä»¥ä¾¿read_argsèƒ½æ­£ç¡®è¯»å–é…ç½®æ–‡ä»¶
    original_argv = sys.argv.copy()
    sys.argv = [sys.argv[0], config_path]
    
    # è¯»å–å‚æ•°
    args = read_args()
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
    
    # æ¢å¤åŸå§‹å‘½ä»¤è¡Œå‚æ•°
    sys.argv = original_argv
    
    # è®¾ç½®remove_unused_columns=Falseï¼Œè¿™å¯¹æˆå¯¹æ•°æ®é›†å¾ˆé‡è¦
    training_args.remove_unused_columns = False
    
    # æ‰“å°å…³é”®å‚æ•°
    print_section("å…³é”®è®­ç»ƒå‚æ•°")
    print(f"æ¨¡å‹: {model_args.model_name_or_path}")
    print(f"æ•°æ®é›†: {data_args.dataset}")
    print(f"è®­ç»ƒé˜¶æ®µ: {finetuning_args.stage}")
    print(f"å¾®è°ƒç±»å‹: {finetuning_args.finetuning_type}")
    print(f"åå¥½betaå€¼: {finetuning_args.pref_beta}")
    print(f"æ‰¹å¤„ç†å¤§å°: {training_args.per_device_train_batch_size}")
    print(f"å­¦ä¹ ç‡: {training_args.learning_rate}")
    print(f"è®­ç»ƒè½®æ¬¡: {training_args.num_train_epochs}")
    print(f"è¾“å‡ºç›®å½•: {training_args.output_dir}")
    
    return model_args, data_args, training_args, finetuning_args, generating_args

def prepare_tokenizer_and_model(model_args, finetuning_args, data_args, training_args, do_train=True):
    """
    å‡†å¤‡tokenizerã€æ¨¡æ¿å’Œæ¨¡å‹
    
    Args:
        model_args: æ¨¡å‹å‚æ•°
        finetuning_args: å¾®è°ƒå‚æ•°
        data_args: æ•°æ®å‚æ•°
        training_args: è®­ç»ƒå‚æ•°
        do_train: æ˜¯å¦è¿›è¡Œè®­ç»ƒ
        
    Returns:
        tokenizerã€templateã€modelåŠç›¸å…³æ¨¡å—
    """
    print_section("å‡†å¤‡Tokenizerå’Œæ¨¡å‹")
    
    # åŠ è½½tokenizer
    tokenizer_module = load_tokenizer(model_args)
    tokenizer = tokenizer_module["tokenizer"]
    print(f"Tokenizerç±»å‹: {tokenizer.__class__.__name__}")
    
    # è·å–æ¨¡æ¿å¹¶ä¿®å¤tokenizer
    template = get_template_and_fix_tokenizer(tokenizer, data_args)
    print(f"ä½¿ç”¨æ¨¡æ¿: {data_args.template}")
    
    # åŠ è½½æ¨¡å‹
    print(f"åŠ è½½æ¨¡å‹: {model_args.model_name_or_path}")
    model = load_model(tokenizer, model_args, finetuning_args, do_train)
    print(f"æ¨¡å‹ç±»å‹: {model.__class__.__name__}")
    
    # åˆ›å»ºå‚è€ƒæ¨¡å‹
    print("å‡†å¤‡å‚è€ƒæ¨¡å‹...")
    if finetuning_args.use_ref_model:
        if finetuning_args.ref_model is None and (not do_train):
            print("ä½¿ç”¨ä¸»æ¨¡å‹ä½œä¸ºå‚è€ƒæ¨¡å‹")
            ref_model = model
        else:
            print(f"ä½¿ç”¨æŒ‡å®šçš„å‚è€ƒæ¨¡å‹: {finetuning_args.ref_model or 'åŸºäºä¸»æ¨¡å‹åˆ›å»º'}")
            ref_model = create_ref_model(model_args, finetuning_args)
    else:
        print("ä¸ä½¿ç”¨å‚è€ƒæ¨¡å‹")
        ref_model = None
    
    return tokenizer, template, model, ref_model, tokenizer_module

def prepare_dataset(template, model_args, data_args, training_args, tokenizer_module):
    """
    å‡†å¤‡æ•°æ®é›†
    
    Args:
        template: æ¨¡æ¿
        model_args: æ¨¡å‹å‚æ•°
        data_args: æ•°æ®å‚æ•°
        training_args: è®­ç»ƒå‚æ•°
        tokenizer_module: tokenizeræ¨¡å—
        
    Returns:
        å¤„ç†åçš„æ•°æ®é›†å’Œæ•°æ®æ•´ç†å™¨
    """
    print_section("å‡†å¤‡æ•°æ®é›†")
    
    # è·å–æ•°æ®é›†ï¼Œåœ¨DPOä¸­ä½¿ç”¨'rm'é˜¶æ®µçš„æ•°æ®å¤„ç†æ–¹å¼
    dataset_module = get_dataset(
        template, 
        model_args, 
        data_args, 
        training_args, 
        stage="rm",  # DPOä½¿ç”¨RMé˜¶æ®µçš„æ•°æ®å¤„ç†é€»è¾‘
        **tokenizer_module
    )
    
    # æ‰“å°æ•°æ®é›†ä¿¡æ¯
    if "train_dataset" in dataset_module:
        train_size = len(dataset_module["train_dataset"])
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {train_size}")
        
        # æ‰“å°ä¸€ä¸ªæ ·æœ¬ç¤ºä¾‹
        if train_size > 0:
            print("\nè®­ç»ƒæ ·æœ¬ç¤ºä¾‹:")
            sample = dataset_module["train_dataset"][0]
            for k, v in sample.items():
                if k.endswith("_ids") or k.endswith("_mask"):
                    print(f"  {k}: [å¼ é‡, é•¿åº¦={len(v)}]")
                else:
                    print(f"  {k}: {v}")
    
    if "eval_dataset" in dataset_module and dataset_module["eval_dataset"] is not None:
        print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(dataset_module['eval_dataset'])}")
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    tokenizer = tokenizer_module["tokenizer"]
    model = dataset_module.get("model", None)
    
    data_collator = PairwiseDataCollatorWithPadding(
        template=template,
        model=model,
        pad_to_multiple_of=8,
        label_pad_token_id=IGNORE_INDEX if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id,
        **tokenizer_module,
    )
    print("åˆ›å»ºäº†PairwiseDataCollatorWithPaddingæ•°æ®æ•´ç†å™¨")
    
    return dataset_module, data_collator

def setup_trainer(
    model, 
    ref_model, 
    training_args, 
    finetuning_args, 
    data_collator, 
    dataset_module, 
    tokenizer_module,
    callbacks
):
    """
    è®¾ç½®DPOè®­ç»ƒå™¨
    
    Args:
        model: æ¨¡å‹
        ref_model: å‚è€ƒæ¨¡å‹
        training_args: è®­ç»ƒå‚æ•°
        finetuning_args: å¾®è°ƒå‚æ•°
        data_collator: æ•°æ®æ•´ç†å™¨
        dataset_module: æ•°æ®é›†æ¨¡å—
        tokenizer_module: tokenizeræ¨¡å—
        callbacks: å›è°ƒå‡½æ•°åˆ—è¡¨
        
    Returns:
        DPOè®­ç»ƒå™¨
    """
    print_section("è®¾ç½®DPOè®­ç»ƒå™¨")
    
    # åˆå§‹åŒ–DPOè®­ç»ƒå™¨
    trainer = CustomDPOTrainer(
        model=model,
        ref_model=ref_model,
        args=training_args,
        finetuning_args=finetuning_args,
        data_collator=data_collator,
        callbacks=callbacks,
        **dataset_module,
        **tokenizer_module,
    )
    
    print(f"åˆ›å»ºäº†CustomDPOTrainerï¼Œè®­ç»ƒè®¾å¤‡: {training_args.device}")
    return trainer

def run_dpo_training(trainer, training_args, finetuning_args, dataset_module=None):
    """
    æ‰§è¡ŒDPOè®­ç»ƒ
    
    Args:
        trainer: è®­ç»ƒå™¨
        training_args: è®­ç»ƒå‚æ•°
        finetuning_args: å¾®è°ƒå‚æ•°
        dataset_module: æ•°æ®é›†æ¨¡å—ï¼Œç”¨äºè®¡ç®—æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
    """
    print_section("æ‰§è¡ŒDPOè®­ç»ƒ")
    
    if training_args.do_train:
        print("å¼€å§‹è®­ç»ƒ...")
        train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
        
        print("ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        trainer.save_state()
        
        # è®°å½•æŒ‡æ ‡
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        
        # æ·»åŠ æœ‰æ•ˆtokenå¤„ç†é€Ÿåº¦æŒ‡æ ‡
        if finetuning_args.include_effective_tokens_per_second and dataset_module and "train_dataset" in dataset_module:
            metrics["effective_tokens_per_sec"] = calculate_tps(
                dataset_module["train_dataset"], metrics, stage="rm"
            )
            print(f"æœ‰æ•ˆtokenå¤„ç†é€Ÿåº¦: {metrics['effective_tokens_per_sec']:.2f} tokens/sec")
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        if finetuning_args.plot_loss:
            print("ç»˜åˆ¶æŸå¤±æ›²çº¿...")
            plot_loss(training_args.output_dir, keys=["loss", "eval_loss", "rewards/accuracies"])
        
        # è®¡ç®—è®­ç»ƒæ­¥æ•°
        train_steps = None
        if hasattr(trainer.state, "global_step"):
            train_steps = trainer.state.global_step
        elif "train_steps_per_second" in metrics and "train_runtime" in metrics:
            # å¦‚æœæ²¡æœ‰ç›´æ¥çš„æ­¥æ•°ï¼Œå¯ä»¥ä»æ¯ç§’æ­¥æ•°å’Œæ€»è¿è¡Œæ—¶é—´è®¡ç®—
            train_steps = round(metrics["train_steps_per_second"] * metrics["train_runtime"])
        elif "train_samples_per_second" in metrics and "train_runtime" in metrics:
            # æˆ–è€…ä»æ¯ç§’æ ·æœ¬æ•°ã€æ‰¹é‡å¤§å°å’Œæ€»è¿è¡Œæ—¶é—´ä¼°ç®—
            effective_batch_size = (
                training_args.per_device_train_batch_size * 
                training_args.gradient_accumulation_steps * 
                (training_args.n_gpu if hasattr(training_args, "n_gpu") and training_args.n_gpu > 0 else 1)
            )
            train_steps = round(
                (metrics["train_samples_per_second"] * metrics["train_runtime"]) / effective_batch_size
            )
        
        print(f"è®­ç»ƒå®Œæˆï¼è®­ç»ƒæ­¥æ•°: {train_steps or 'æœªèƒ½è®¡ç®—'}")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {metrics.get('train_runtime', 'æœªçŸ¥')} ç§’")
        print(f"å¹³å‡æ¯æ­¥æ—¶é—´: {1.0/metrics.get('train_steps_per_second', 0) if metrics.get('train_steps_per_second', 0) > 0 else 'æœªçŸ¥'} ç§’")
    
    # è¯„ä¼°
    if training_args.do_eval:
        print("å¼€å§‹è¯„ä¼°...")
        metrics = trainer.evaluate(metric_key_prefix="eval")
        
        # å¦‚æœå‚è€ƒæ¨¡å‹å°±æ˜¯æ¨¡å‹æœ¬èº«ï¼Œæ— æ³•è®¡ç®—å¥–åŠ±æŒ‡æ ‡
        model = trainer.model
        ref_model = trainer.ref_model
        if id(model) == id(ref_model):
            print("å‚è€ƒæ¨¡å‹ä¸ä¸»æ¨¡å‹ç›¸åŒï¼Œæ— æ³•è®¡ç®—å¥–åŠ±æŒ‡æ ‡")
            remove_keys = [key for key in metrics.keys() if "rewards" in key]
            for key in remove_keys:
                metrics.pop(key)
        
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)
        print(f"è¯„ä¼°å®Œæˆï¼è¯„ä¼°æŒ‡æ ‡: {metrics}")

def create_callbacks(model_args, data_args, training_args, finetuning_args, generating_args):
    """
    åˆ›å»ºè®­ç»ƒå›è°ƒå‡½æ•°
    
    Args:
        model_args: æ¨¡å‹å‚æ•°
        data_args: æ•°æ®å‚æ•°
        training_args: è®­ç»ƒå‚æ•°
        finetuning_args: å¾®è°ƒå‚æ•°
        generating_args: ç”Ÿæˆå‚æ•°
        
    Returns:
        å›è°ƒå‡½æ•°åˆ—è¡¨
    """
    callbacks = []
    
    # æ·»åŠ æ—¥å¿—å›è°ƒ
    callbacks.append(LogCallback())
    
    # æ·»åŠ PISSAè½¬æ¢å›è°ƒï¼Œå¦‚æœå¯ç”¨
    if finetuning_args.pissa_convert:
        callbacks.append(PissaConvertCallback())
    
    # æ·»åŠ SwanLabå›è°ƒï¼Œå¦‚æœå¯ç”¨
    if hasattr(finetuning_args, 'use_swanlab') and finetuning_args.use_swanlab:
        from llamafactory.train.trainer_utils import get_swanlab_callback
        callbacks.append(get_swanlab_callback(finetuning_args))
    
    # æ·»åŠ Reporterå›è°ƒ
    callbacks.append(ReporterCallback(model_args, data_args, finetuning_args, generating_args))
    
    return callbacks
    
def run_dpo_workflow(config_path: str):
    """
    å®Œæ•´çš„DPOè®­ç»ƒå·¥ä½œæµç¨‹
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    """
    try:
        # å¤„ç†å‚æ•°
        model_args, data_args, training_args, finetuning_args, generating_args = process_args(config_path)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºDPOè®­ç»ƒ
        if finetuning_args.stage != "dpo":
            raise ValueError(f"é…ç½®æ–‡ä»¶æŒ‡å®šçš„è®­ç»ƒé˜¶æ®µä¸æ˜¯DPOï¼Œè€Œæ˜¯: {finetuning_args.stage}")
        
        # åˆ›å»ºå›è°ƒå‡½æ•°
        callbacks = create_callbacks(model_args, data_args, training_args, finetuning_args, generating_args)
        
        # å‡†å¤‡tokenizerå’Œæ¨¡å‹
        tokenizer, template, model, ref_model, tokenizer_module = prepare_tokenizer_and_model(
            model_args, finetuning_args, data_args, training_args, training_args.do_train
        )
        
        # å‡†å¤‡æ•°æ®é›†
        dataset_module, data_collator = prepare_dataset(
            template, model_args, data_args, training_args, tokenizer_module
        )
        
        # è®¾ç½®è®­ç»ƒå™¨
        trainer = setup_trainer(
            model, ref_model, training_args, finetuning_args, 
            data_collator, dataset_module, tokenizer_module, callbacks
        )
        
        # æ‰§è¡Œè®­ç»ƒ
        run_dpo_training(trainer, training_args, finetuning_args, dataset_module)
        
        print("\nğŸ‰ DPOè®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"DPOè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise

def main():
    """
    ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶å¯åŠ¨è®­ç»ƒ
    """
    setup_logging()
    
    # é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
    default_config = "examples/train_lora/qwen1_5_0_5b_lora_dpo_test.yaml"
    
    # ä»å‘½ä»¤è¡Œè·å–é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœ‰çš„è¯
    if len(sys.argv) > 1:
        config_path = sys.argv[1]
    else:
        config_path = default_config
        logger.info(f"æœªæŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {default_config}")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)
    
    # åŠ è½½é…ç½®é¢„è§ˆ
    _ = load_config_file(config_path)
    
    # æ‰§è¡ŒDPOè®­ç»ƒ
    run_dpo_workflow(config_path)

if __name__ == "__main__":
    main() 


# è¿è¡Œå‘½ä»¤
# python dpo_baseline/run_dpo_detailed.py examples/train_lora/qwen1_5_0_5b_lora_dpo_test.yaml

# è¿è¡Œç»“æœ
# åŠ è½½é…ç½®æ–‡ä»¶: examples/train_lora/qwen1_5_0_5b_lora_dpo_test.yaml
# é…ç½®å†…å®¹é¢„è§ˆ:
