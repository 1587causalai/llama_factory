# 计算 Beta 的网络

在标准的 DPO (Direct Preference Optimization)  算法中，一个关键的超参数是 $\beta$，它控制着模型在参考策略和奖励信息之间的权衡。从信息融合的角度来看：

- 较大的 $\beta$ 值使模型更倾向于遵循参考策略 $\pi_{\text{ref}}$，保持原有行为
- 较小的 $\beta$ 值使模型更多地利用奖励信息，进行策略调整

然而，传统的 DPO 通常使用固定的 $\beta$ 值，这带来了两个主要局限：

1. **上下文不敏感**：不同场景下可能需要不同的探索-利用权衡
   - 在模型熟悉的领域，应该更多地保持参考策略的行为
   - 在模型不熟悉的领域，应该更多地从奖励信息中学习

2. **优化效率受限**：固定的权衡策略可能导致
   - 在某些场景下过度保守，错过学习机会
   - 在某些场景下过度激进，损失已有能力

这种"一刀切"的方式无法针对不同的上下文动态调整学习策略，限制了模型在复杂、多变场景下的优化效果。

因此我们开发一个基于 Learnable Beta DPO 的人类偏好对齐微调框架，通过自适应调整 DPO 算法中的 $\beta$ 参数来实现更精细的探索-利用平衡控制。核心思想是设计一个可学习的函数：

$$\beta(x) = w \cdot \log(PPL(x)) \cdot f(x)$$

其中：
- $w$ 是一个可学习的参数, 可以进行合适的初始化, 具备可解释可操作性, 感觉扮演一个类似 beta 量纲的作用. 
- $\mathrm{PPL}(x)$ 是上下文 $x$ 的困惑度, PPL 越大，表示模型越困惑，越难预测下一个词。使用策略模型 $\pi_\theta$ 计算, $\mathrm{PPL}_{\pi_\theta}(x)$ 反映了**策略模型对输入的确定性程度**。对于给定的输入序列 $x = (x_1, x_2, ..., x_m)$，困惑度定义为：
$$\mathrm{PPL}_{\pi_\theta}(x) = \exp \left( - \frac{1}{m} \sum_{i=1}^m \log \pi_\theta(x_i | x_{<i}) \right)$$
- $f(x)$ 是上下文 $x$ 的函数，其取值范围为 $[1-\epsilon, 1+\epsilon]$, 具体实现中，
$$f(x) = 1 + \epsilon \cdot \tanh(NN(h_{\pi_\theta}(x)))$$  
其中 $h_{\pi_\theta}(x)$ 是由策略模型 $\pi_\theta$ 得到的最后一层隐状态，$NN(h_{\pi_\theta}(x))$ 是一个神经网络. 


请注意本质上, 负对数似然（NLL）和困惑度（PPL）之间的关系：$\log(PPL(x)) = \frac{NLL(x)}{m}$， 其中 $m$ 是输入序列的长度, 所以我们可以简化计算。


具体的网络设计的基本思路是：为 base LLM 新增一个输出层，用于计算模型对 current prompt 的了解程度, 决定该探索还是利用已有知识。
- $w$ 参数默认是可梯度更新的 nn.parameter.Parameter, 方便消融实验, 也可以固定为某个常数, 引入一个参数控制.   
- 第一个子网络 (MLP or other) 接在 base LLM 的最后一层隐状态后面, 计算得到 $f(h) = 1 + \epsilon \cdot \tanh(NN(h))$, 引入一个参数控制 $\epsilon$ 的取值. 
- 第二个子网络 (MLP or other) 接在 base LLM 的最后一层隐状态后面, 作为一个战略储备用于近似计算 $log(PPL)$ (TODO, 需要经过实验验证是否可行, 是否计算效率更高, 我觉得可以引入一个参数控制是否使用该子网络)





