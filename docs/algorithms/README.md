# LLaMA Factory微调算法前向传播分析

本目录包含对LLaMA Factory中各种微调算法前向传播过程的详细分析。通过对比不同算法的实现细节，我们可以更好地理解它们的异同，为定制和优化算法提供参考。

## 收录的算法

我们计划分析以下算法的前向传播实现：

1. **SFT (Supervised Fine-Tuning)** - 监督式微调
   - 文件：[sft_forward_pass.md](sft_forward_pass.md)
   - 状态：✅ 已完成

2. **DPO (Direct Preference Optimization)** - 直接偏好优化
   - 文件：[dpo_forward_pass.md](dpo_forward_pass.md)
   - 状态：✅ 已完成

3. **PPO (Proximal Policy Optimization)** - 近端策略优化
   - 文件：[ppo_forward_pass.md](ppo_forward_pass.md)
   - 状态：✅ 已完成

4. **KTO (KL-constrained preference Optimization)** - KL约束偏好优化
   - 文件：[kto_forward_pass.md](kto_forward_pass.md)
   - 状态：✅ 已完成

5. **PT (Post Training)** - 后训练
   - 文件：[../pt_forward_pass_analysis.md](../pt_forward_pass_analysis.md)
   - 状态：✅ 已完成

## 分析维度

每种算法的分析从以下几个维度展开：

1. **算法原理概述** - 简单介绍算法的基本原理和目标
2. **数据流和输入格式** - 输入数据的处理和格式要求
3. **前向传播实现** - 详细分析模型计算过程
4. **损失函数计算** - 算法特有的损失函数实现
5. **与其他算法的区别** - 重点对比不同算法的异同

## 算法对比总结

### 输入数据格式

| 算法 | 数据格式 | 特殊要求 |
|------|---------|---------|
| SFT  | 单一回复 | 输入输出对 |
| DPO  | 偏好对（选中/拒绝） | 需要人类偏好标注 |
| KTO  | 偏好对 + KL样本 | 需要专门的KL计算样本 |
| PPO  | 生成多个回复 + 奖励分数 | 需要奖励模型评分 |

### 损失函数特点

| 算法 | 主要损失 | 特点 |
|------|---------|------|
| SFT  | 负对数似然损失 | 简单直接的模仿学习 |
| DPO  | 偏好损失+隐式KL约束 | 无需显式奖励模型 |
| KTO  | 偏好损失+显式KL约束 | 更稳定的训练过程 |
| PPO  | 策略梯度+KL惩罚 | 完整的强化学习流程 |

### 训练复杂度

| 算法 | 计算复杂度 | 训练稳定性 | 超参数敏感度 |
|------|----------|-----------|------------|
| SFT  | 低        | 高        | 低         |
| DPO  | 中        | 中        | 中         |
| KTO  | 中        | 高        | 低         |
| PPO  | 高        | 低        | 高         |

## 使用指南

这些文档主要用于研究和学习目的，帮助理解LLaMA Factory中各算法的实现细节。您可以：

- 单独阅读感兴趣的算法分析
- 对比不同算法的实现差异
- 基于分析结果定制自己的算法

## 算法总结与适用场景

### SFT (监督式微调)
- **特点**：最基础的微调方法，直接学习输入到输出的映射
- **优势**：实现简单，训练稳定，计算资源需求低
- **劣势**：无法学习复杂的人类偏好，容易过拟合训练数据
- **适用场景**：
  - 有高质量的输入-输出对数据
  - 任务明确且回答模式相对固定
  - 作为其他高级微调方法的基础阶段
  - 计算资源有限的环境

### DPO (直接偏好优化)
- **特点**：通过偏好对直接优化模型，避免显式奖励模型
- **优势**：比PPO更高效，无需复杂的RL训练流程
- **劣势**：隐式KL约束可能导致训练不稳定
- **适用场景**：
  - 有大量人类偏好标注数据
  - 需要模型学习细微的质量差异
  - 计算资源中等但希望获得RLHF类似效果
  - 对齐任务中需要平衡模型能力和人类偏好

### KTO (KL约束偏好优化)
- **特点**：DPO的改进版，添加显式KL约束
- **优势**：训练更稳定，对超参数不那么敏感
- **劣势**：实现略复杂，需要额外的KL样本
- **适用场景**：
  - DPO训练不稳定的情况
  - 需要更精确控制模型偏离参考模型的程度
  - 对模型行为有更严格约束的场景
  - 大规模训练中需要更好稳定性的情况

### PPO (近端策略优化)
- **特点**：完整的强化学习流程，通过奖励信号优化策略
- **优势**：理论上可以优化任何可量化的目标
- **劣势**：计算复杂度高，训练不稳定，超参数敏感
- **适用场景**：
  - 有专门的奖励模型或评分机制
  - 需要优化特定且可量化的目标
  - 有充足的计算资源
  - 需要最大程度改变模型行为的场景

### PT (后训练)
- **特点**：在预训练任务上继续训练，扩展模型知识
- **优势**：可以增强模型的基础能力和知识面
- **劣势**：需要大量无标注数据，训练成本高
- **适用场景**：
  - 模型知识需要更新或扩展
  - 领域适应性训练
  - 作为其他微调方法的预备阶段
  - 需要保持模型通用能力的场景

## 算法选择建议

1. **资源受限场景**：优先考虑SFT，其次是DPO
2. **追求最佳性能**：可考虑SFT→DPO/KTO→PPO的渐进式训练
3. **特定领域适应**：PT→SFT的组合可能效果最佳
4. **对齐任务**：
   - 轻度对齐：SFT或DPO
   - 中度对齐：KTO
   - 强度对齐：PPO
5. **训练稳定性优先**：SFT > KTO > DPO > PPO

## 贡献

欢迎对文档进行补充和修正，特别是在以下方面：
- 添加更多算法的分析
- 更新最新版本的实现细节
- 提供算法性能对比和最佳实践 