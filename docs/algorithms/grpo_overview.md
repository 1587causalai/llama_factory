# GRPO (Group Relative Policy Optimization) 算法概述

## 1. 算法简介

GRPO (Group Relative Policy Optimization) 是一种专为大型语言模型优化设计的强化学习算法，它是PPO (Proximal Policy Optimization) 算法的改进版本，主要用于解决模型与人类偏好对齐的问题。GRPO 由 DeepSeek AI 研究团队提出并应用于 DeepSeek 系列模型的训练中，特别是在 DeepSeek-Math 和 DeepSeek-R1 等模型上取得了显著成果。相比传统的 PPO 算法，它具有更高的效率和稳定性。

GRPO 的核心创新点是使用**群组相对优势估计**来替代传统的值函数模型，并将 KL 散度约束直接整合到损失函数中，从而简化了训练过程并减少了计算资源的需求。

## 2. GRPO 算法的核心创新

### 2.1 基于组的相对优势估计

在传统的 PPO 算法中，需要一个单独的值函数模型（critic网络）来估计状态价值，这不仅增加了内存开销，还增加了计算复杂度。GRPO 提出了一种新的优势估计方法：

- 对于给定的问题/提示（prompt），模型生成一组（通常是 G 个）不同的响应
- 使用奖励模型评估每个响应的质量，得到相应的奖励值
- 以组内所有响应的平均奖励作为基线（baseline）
- 计算每个响应相对于这个基线的归一化优势值：`优势值 = (响应的奖励 - 组平均奖励) / 组奖励的标准差`

这种基于组的优势估计方法消除了对独立值函数网络的需求，大大降低了内存和计算成本。同时，这种方法也与奖励模型的比较性质很好地匹配，因为奖励模型通常是在相同问题的输出比较数据集上训练的。

### 2.2 KL 散度的直接整合

GRPO 将当前策略与参考策略（通常是 SFT 模型）之间的 KL 散度直接整合到损失函数中：

- 这种整合确保了策略更新的稳定性，防止模型偏离太远
- 避免了传统 PPO 中裁剪技术可能带来的优化问题
- 提供了更平滑的训练曲线和更稳定的收敛行为

## 3. GRPO 算法在 LLM 训练中的应用

GRPO 算法在大语言模型训练中的应用流程如下：

### 3.1 训练流程
1. **监督微调 (SFT)** - 首先使用高质量的人类示范数据集对模型进行微调，产生基础的 SFT 模型
2. **奖励建模** - 训练一个奖励模型，用于评估模型输出的质量
3. **GRPO 优化** - 使用 GRPO 算法基于奖励模型进行策略优化

### 3.2 GRPO 优化具体步骤
1. **组采样** - 对于给定的问题，模型生成多个响应
2. **奖励评分** - 奖励模型评估每个响应的质量
3. **优势计算** - 计算每个响应相对于组平均奖励的归一化优势值
4. **策略更新** - 调整模型参数，提高高奖励响应的生成概率，同时使用 KL 散度约束防止策略偏离太远
5. **迭代训练** - 重复以上过程，逐步提高模型生成高质量、符合人类偏好的文本的能力

## 4. GRPO 与 PPO 的比较

### 4.1 效率
- **内存使用**：GRPO 通过消除值函数网络显著减少了内存开销
- **计算成本**：基于组的优势估计简化了计算流程，降低了计算复杂度

### 4.2 稳定性
- **训练稳定性**：KL 散度的直接整合和组相对优势估计提供了更稳定的训练行为
- **收敛性能**：相比 PPO，GRPO 通常能够更快速稳定地收敛

### 4.3 可扩展性
- GRPO 更适合大规模模型（如 DeepSeek-Math、DeepSeek-R1）的训练，资源效率更高
- 对于极大规模的模型训练，GRPO 的优势更为明显

## 5. GRPO 在实际应用中的表现

EasyR1 是一个基于 veRL 项目的高效多模态强化学习训练框架，它支持 GRPO 算法并已在多种模型上得到验证，包括：

- Qwen2/Qwen2.5 语言模型
- Qwen2/Qwen2.5-VL 视觉语言模型
- DeepSeek-R1 蒸馏模型

EasyR1 支持各种文本和视觉-文本数据集，为 GRPO 算法的实际应用提供了强大的工具支持。

## 6. GRPO 的数学原理

### 6.1 LLM作为策略

在GRPO中，语言模型作为策略网络（actor），接收问题q作为输入观察s，并产生一系列标记作为动作。策略分布可以表示为：

$$π_θ(a|q) = ∏_{t=1}^N π_θ(a_t | q, a_{<t})$$

语言模型的自回归性质决定了每个标记的生成都依赖于之前生成的标记。

### 6.2 奖励和优势计算

对于每个生成的序列，GRPO计算每个标记的奖励如下：

$$r_t = r_ϕ(q, a_{≤t}) - β log(π_θ(a_t | q, a_{<t}) / π_ref(a_t | q, a_{<t}))$$

GRPO通过对同一问题生成的多个不同输出的奖励进行归一化来估计优势，而不是使用值函数网络：

$$Â_{i,t} = (r_i - mean(r)) / std(r)$$

### 6.3 GRPO目标函数

GRPO的完整目标函数如下：

$$J_GRPO(θ) = (1/G)∑_{i=1}^G (1/|a_i|)∑_{t=1}^{|a_i|} {min[(π_θ(a_{i,t} | s, a_{i,<t}) / π_{θ_old}(a_{i,t} | s, a_{i,<t}))Â_{i,t}, clip(π_θ(a_{i,t} | s, a_{i,<t}) / π_{θ_old}(a_{i,t} | s, a_{i,<t}), 1-ε, 1+ε)Â_{i,t}] - β D_KL[π_θ || π_ref]}$$      

这个目标函数：
1. 对组和序列长度进行平均
2. 使用裁剪来实现保守更新
3. 包含KL散度惩罚项，防止策略偏离参考模型太远

## 7. 总结

GRPO 算法通过群组相对优势估计和 KL 散度的直接整合，成功解决了传统 PPO 算法在大型语言模型训练中的效率和稳定性问题。它的设计理念特别适合大规模语言模型的人类偏好对齐训练，已在 DeepSeek-Math 和 DeepSeek-R1 等先进模型的训练中得到验证，取得了显著成果。

虽然 LLaMA Factory 项目目前直接支持 SFT、DPO、KTO、PPO、RM 和 PT 这些算法，但 GRPO 算法已在相关的 EasyR1 项目中实现并开源，为研究人员和开发者提供了另一种高效的强化学习从人类反馈（RLHF）解决方案。

## 参考资料

[1] Schulman, J., et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, 2017.
[2] Shao, Z., et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300, 2024.
[3] DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, 2025. 