# LEDPO 算法实验研究总结

本文档总结了我们针对LEDPO算法进行的一系列实验研究，包括实验设计、目的和结果概述。

## 已完成实验

### 1. 样本数量影响实验 (`sample_size_impact/`)

**实验目的**：研究样本数量对DPO算法性能的影响，包括训练/评估准确率、损失和beta变化等指标。

**实验设计**：
- 使用不同数量的训练样本(500, 1000, 2000等)
- 控制其他变量保持不变
- 观察训练收敛性和最终性能

**主要发现**：
- 样本数量增加通常能提高模型性能
- 存在样本数量与收敛速度的权衡关系

### 2. 固定beta影响实验 (`fixed_beta_impact/`)

**实验目的**：探究不同固定beta值对LEDPO算法性能的影响。

**实验设计**：
- 测试不同的beta值(0.1, 0.5, 0.73等)
- 使用相同的模型和数据集
- 分析beta值对训练稳定性和性能的影响

**主要发现**：
- beta值影响训练的稳定性和最终性能
- 存在某个最优beta值区间

### 3. 激活函数影响实验 (`activation_fn_impact/`)

**实验目的**：比较ValueHead网络中不同激活函数对动态beta计算的影响。

**实验设计**：
- 比较三种激活函数：Sigmoid, Softplus, ReLU
- 测试不同缩放因子：beta_scale=1.0和beta_scale=10.0
- 同时对比固定beta和动态beta的性能差异
- 使用2000个样本进行训练

**主要发现**：
- 不同激活函数产生不同范围的beta值
- 激活函数的选择影响模型的训练稳定性和收敛速度

### 4. 模型对比实验 (`model_impact/`)

**实验目的**：研究不同基础模型和beta设置对LEDPO算法效果的影响。

**实验设计**：
- 比较三种不同模型：
  - Qwen1.5-0.5B：较小的模型，0.5B参数规模
  - Qwen2.5-1.5B-Instruct：Qwen系列的2.5代模型，1.5B参数规模
  - DeepSeek-R1-Distill-Qwen-1.5B：DeepSeek蒸馏的Qwen模型，1.5B参数规模
- 对每个模型分别测试固定beta和动态beta两种设置
- 使用相同的数据集(HH-RLHF)和样本数(2000个)

**主要发现**：
- 模型规模和架构对LEDPO算法性能有显著影响
- 不同模型适合不同的beta设置策略

## 实验分析和结论

通过上述实验，我们得到了以下主要发现：

1. **样本数量影响**：更多的样本通常能提高模型性能，但存在收益递减现象。

2. **beta参数敏感性**：
   - beta值直接影响训练的稳定性和最终性能
   - 动态beta在某些情况下优于固定beta，特别是对于较大的模型

3. **激活函数选择**：
   - Sigmoid限制beta值在固定范围内，有助于训练稳定
   - ReLU和Softplus允许更大的beta值范围，可能在某些场景下更灵活

4. **模型规模影响**：
   - 更大的模型通常表现更好，但计算成本也更高
   - 不同模型对beta参数的敏感度不同

## 下一步实验方向

基于已有实验结果，我们提出以下可能的后续实验方向：

### 1. 数据质量影响实验

**实验设计**：
- 比较不同质量的偏好数据对LEDPO算法的影响
- 可以使用不同的数据集，如Anthropic-HH、Stanford-SHP等
- 或者尝试对数据集进行质量筛选/分层

### 2. 优化器和学习率策略实验

**实验设计**：
- 测试不同优化器(AdamW, 8bit-Adam, Lion等)对LEDPO的影响
- 比较不同学习率策略(线性、余弦衰减等)
- 研究优化器参数(如weight decay)对训练稳定性的影响

### 3. 混合策略实验

**实验设计**：
- 结合SFT、DPO和其他策略(如RLHF、PPO)的混合训练方法
- 测试不同阶段切换策略的效果
- 探索多任务学习框架下的偏好优化

### 4. 跨领域泛化性实验

**实验设计**：
- 在一个领域数据上训练LEDPO模型
- 在不同领域数据上评估模型性能
- 研究领域自适应技术以提高跨领域泛化性

### 5. 长序列处理能力实验

**实验设计**：
- 研究LEDPO在不同长度序列上的表现
- 比较不同上下文长度设置对训练效果的影响
- 探索针对长文本的优化技术

这些实验将帮助我们更全面地理解LEDPO算法，并进一步优化其在不同场景下的应用。 