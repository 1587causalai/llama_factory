# fooDPO 算法开发日志

## 项目概述

这个项目旨在实现一个fooDPO (Food Direct Preference Optimization) 算法，它是DPO (Direct Preference Optimization) 的一个改进版本。fooDPO通过引入动态beta值调整、特殊的foo_factor因子和样本级别的动态权重计算，提高模型从人类偏好数据中学习的能力。

## 日志

### 2025-03-05：项目初始化与fooDPO设计

- 从`dpo_baseline`项目创建了`foodpo_baseline`目录结构
- 设计了fooDPO算法的核心创新点：
  - 动态beta值调整：根据样本复杂度动态调整beta值
  - foo_factor：引入特殊的权重调整因子
  - 动态权重：基于样本特性调整不同样本的训练权重

### 旧项目参考（DPO基线）

- DPO是一种直接从人类偏好数据中优化语言模型的方法
- 不同于RLHF需要训练额外的奖励模型，DPO直接从偏好数据中计算隐式奖励
- 基本数学公式：最大化偏好数据中"被选中回答"的概率，同时最小化"被拒绝回答"的概率

### 待完成任务

1. **实现fooDPO算法核心**：
   - 修改现有的DPO Trainer，添加动态beta值调整功能
   - 实现foo_factor的计算和应用逻辑
   - 添加样本级别的动态权重计算

2. **扩展训练过程**：
   - 在标准的forward-loss-backward流程中加入动态权重计算阶段
   - 记录每个样本的实际使用beta值和权重，用于后续分析

3. **评估与比较**：
   - 与标准DPO算法进行对比实验
   - 分析不同参数设置（beta_scale、foo_factor）对训练效果的影响

4. **算法优化**：
   - 基于初步实验结果，进一步优化fooDPO算法
   - 探索其他可能的改进方向 