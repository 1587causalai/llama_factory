# Learnable Beta DPO：一种自适应探索-利用的DPO算法

## 1. 引言

### 1.1 背景与动机

Direct Preference Optimization (DPO) 作为一种直接优化语言模型以对齐人类偏好的新兴算法，因其简洁高效而备受关注。相较于传统强化学习方法，DPO 避免了复杂的奖励建模和策略迭代，通过直接比较模型对 chosen 和 rejected 样本的输出进行优化。然而，标准 DPO 采用固定的超参数 $\beta$ 来平衡参考策略和偏好学习，这限制了其在复杂场景下的优化潜力。

**固定 $\beta$ 的局限性：**

1. **上下文不敏感性:** 固定的探索-利用权衡策略无法适应不同输入上下文的需求。模型在熟悉领域应侧重保守的策略保持，而在不熟悉领域则需加大探索，从偏好数据中充分学习。
2. **优化效率瓶颈:**  "一刀切"的 $\beta$ 值可能导致在某些情境下学习保守而错失优化机会，或在另一些情境下学习激进而损害已有能力，最终降低整体优化效率。

### 1.2 Learnable Beta DPO 的核心思想：动态偏好强度调制

为克服上述局限，本文提出 Learnable Beta DPO，其核心思想是引入**上下文相关的动态 $\beta(x)$**，使模型能够根据输入 $x$ 自适应地调整偏好学习的强度。通过学习函数 $\beta(x; \pi_\theta)$，模型能够精细化控制 preference loss 的影响，从而更有效地学习人类偏好，提升模型在复杂多变场景下的优化效果。


## 2. 标准 DPO 算法回顾

### 2.1 偏好模型：Bradley-Terry 模型

DPO 的理论基础是 Bradley-Terry 模型，用于建模成对偏好关系。对于给定上下文 $x$ 和模型输出对 $(y_w, y_l)$ (winner vs. loser)，Bradley-Terry 模型假设 $y_w$ 比 $y_l$ 更受偏好的概率为：

$$P(\text{winner} = y_w | x, y_w, y_l) = \frac{\exp(r(x, y_w))}{\exp(r(x, y_w)) + \exp(r(x, y_l))}$$

其中 $r(x, y)$ 代表模型输出 $y$ 在上下文 $x$ 下的奖励值。DPO 的目标是在不显式学习奖励函数 $r(x, y)$ 的前提下，直接优化策略模型 $\pi_\theta(y|x)$。

### 2.2 DPO Loss 函数推导

DPO 旨在最大化 chosen 样本的似然，同时最小化 rejected 样本的似然。基于最大似然估计，并假设奖励函数 $r(x, y)$ 与策略模型 $\pi_\theta(y|x)$ 和参考策略 $\pi_{\text{ref}}(y|x)$ 的对数比值成正比：

$$r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}$$

将上述奖励函数代入负对数似然损失函数并简化，得到标准 DPO Loss 函数：

$$\mathcal{L}_{\text{DPO}}(\theta) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \left[ \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right] \right) \right]$$

其中 $\sigma(z) = (1 + e^{-z})^{-1}$ 为 sigmoid 函数，$\mathcal{D}$ 为偏好数据集。


### 2.3 标准 DPO 的训练过程

标准 DPO 的训练过程主要包括：

1.  **数据准备**: 收集偏好数据集，包含上下文 $x^{(i)}$，chosen 样本 $y_w^{(i)}$ 和 rejected 样本 $y_l^{(i)}$。
2.  **模型初始化**: 初始化策略模型 $\pi_\theta$ 和参考模型 $\pi_{ref}$ (通常 $\pi_{ref}$ 是训练前的策略模型)。
3.  **Loss 计算**: 对于每个 batch 的数据，计算 DPO Loss $\mathcal{L}_{\text{DPO}}(\theta)$。
4.  **梯度更新**: 使用梯度下降等优化算法，更新策略模型 $\pi_\theta$ 的参数，最小化 DPO Loss。
5.  **迭代训练**: 重复步骤 3 和 4，直到模型收敛或达到预定的训练步数。

### 2.4 信息融合视角

   使用随机积算子将两种信息融合：
   $$\pi(y|x) = \pi_{\text{ref}}(y|x) \odot p_r(y|x;\beta)$$
   
   这等价于 DPO 的最优策略形式：
   $$\pi(y|x) \propto \pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))$$

- 当 $\beta \to \infty$：$p_{\text{r}}(y|x;\beta)$ 趋近均匀分布, 信息融合后结果等于参考策略, 意味着完全不用奖励信息, 是一种保守的利用过往经验策略.
- 当 $\beta \to 0$：信息融合后直接选择奖励最大的动作, 完全抛弃当前策略信息, 使用奖励信息确定动作, 可以理解成一种完全的利用环境反馈进行探索的策略.
- $\beta$ 实际上控制了探索和利用的平衡.

因此我们希望 $\beta \to \beta(x; \pi_\theta)$ 是一个可以自适应探索和利用的平衡的参数.

## 3. Learnable Beta DPO 数学模型

### 3.1 动态 Beta 函数 $\beta(x; \pi_\theta)$

Learnable Beta DPO 的核心创新是将固定的 $\beta$ 替换为**依赖于上下文 $x$ 和策略模型 $\pi_\theta$ 的动态函数 $\beta(x; \pi_\theta)$**。本文提出一种基于策略模型困惑度和隐状态的 $\beta(x; \pi_\theta)$ 函数：

$$\beta(x; \pi_\theta) = w \cdot \mathrm{PPL}_{\pi_\theta}(x) \cdot f(h_{\pi_\theta}(x))$$

各组成部分的解释如下：

1. **可学习参数 $w$:**  可学习标量，调节 $\beta$ 值的整体尺度。
2. **策略模型困惑度 $\mathrm{PPL}_{\pi_\theta}(x)$:**  度量策略模型对输入 $x$ 的确定性。高困惑度表示模型对输入不确定，反之则确定。计算公式为：
    $$\mathrm{PPL}_{\pi_\theta}(x) = \exp \left( - \frac{1}{m} \sum_{i=1}^m \log \pi_\theta(x_i | x_{<i}) \right)$$
3. **BetaHead 调整函数 $f(h_{\pi_\theta}(x))$:** 基于策略模型最后一层隐状态 $h_{\pi_\theta}(x)$ 的细粒度调整。具体形式为：
    $$f(h) = 1 + \epsilon \cdot \tanh(\mathrm{NN}(h))$$
    其中 $\mathrm{NN}(\cdot)$ 为小型神经网络 (BetaHead)，$\epsilon$ 为小常数，确保调整幅度在 $[1-\epsilon, 1+\epsilon]$ 范围内。

### 3.2 Learnable Beta DPO Loss 函数

将动态 $\beta(x; \pi_\theta)$ 代入标准 DPO Loss 函数，得到 Learnable Beta DPO Loss 函数：

$$\mathcal{L}_{\text{LearnableBetaDPO}}(\theta) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta(x; \pi_\theta) \left[ \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right] \right) \right]$$


## 4. Learnable Beta DPO 训练过程

### 4.1 模型结构：BetaHead 与策略模型协同

Learnable Beta DPO 引入 BetaHead 网络与策略模型紧密耦合，实现动态 $\beta$ 值的计算。

**关键特点：**

1. **共享表征:** BetaHead 直接利用策略模型 $\pi_\theta$ 的最后一层隐状态 $h_{\pi_\theta}(x)$，避免重复特征提取，提升效率。
2. **轻量级 BetaHead:**  $\mathrm{NN}(\cdot)$ 通常为小型神经网络，如线性层或 MLP，输出标量值。
3. **协同学习:** 策略模型参数更新影响 BetaHead 输入，BetaHead 梯度反向传播影响策略模型表征学习，促进二者协同进化。

**模型架构组成：**

* 策略模型 $\pi_\theta$：生成响应、计算 PPL、提供隐状态表征。
* BetaHead 网络：基于隐状态计算动态 $\beta(x; \pi_\theta)$。
* 参考模型 $\pi_{\text{ref}}$：提供基准策略，参数固定。

### 4.2 联合训练流程

Learnable Beta DPO 采用联合训练策略模型 $\pi_\theta$、BetaHead 网络和可学习参数 $w$。

**训练步骤（精简版）：**

1. **初始化:** 初始化策略模型 $\pi_\theta$, BetaHead 网络, 可学习参数 $w$，加载参考模型 $\pi_{\text{ref}}$ (固定)。
2. **前向计算:**
    a. 获取策略模型对输入 $x^{(i)}$ 的隐状态 $h^{(i)}$。
    b. 计算困惑度 $\mathrm{PPL}_{\pi_\theta}(x^{(i)})$。
    c. 通过 BetaHead 计算动态 $\beta(x^{(i)}; \pi_\theta)$。
    d. 计算 Learnable Beta DPO Loss。
3. **梯度更新:**  反向传播梯度，同步更新策略模型 $\pi_\theta$、BetaHead 网络和 $w$ 的参数。
4. **迭代训练:** 重复步骤 2-3 直至收敛。

## 5. 理论分析与优势

### 5.1 动态 Beta 的直觉解释：自适应探索-利用平衡

动态 $\beta(x; \pi_\theta)$ 的核心优势在于实现**上下文自适应的探索-利用平衡**。$\beta$ 值控制模型在参考策略和偏好信息之间的权衡：

* **高 $\beta(x)$:**  倾向于参考策略 $\pi_{\text{ref}}$ (利用)，适用于模型熟悉领域，保持稳定性。
* **低 $\beta(x)$:**  倾向于偏好信息 (探索)，适用于模型不熟悉领域，促进学习。

**自适应机制优势：**

1. **稳定性与灵活性兼顾:**  熟悉场景保持稳定，新颖场景保持灵活。
2. **模型确定性感知:**  通过 $\mathrm{PPL}_{\pi_\theta}(x)$ 感知模型对输入的确定性程度。
3. **细粒度上下文特征学习:** BetaHead 基于隐状态 $h_{\pi_\theta}(x)$ 学习更精细的上下文特征，实现更精准的 $\beta$ 调整。

### 5.2 理论挑战与未来方向

Learnable Beta DPO 虽具潜力，仍面临理论挑战和研究方向：

* **最优 $\beta(x)$ 函数设计:**  当前 $\beta(x)$ 形式为启发式，更优设计方案待探索，例如更复杂的上下文特征或神经网络结构。
* **理论分析:**  动态 $\beta$ 对 DPO 收敛性、稳定性、泛化性的影响尚需深入理论分析，并尝试证明其性能提升的理论依据。
* **与其他动态 $\beta$ 方法比较:**  对比现有动态调整 DPO 中 $\beta$ 值的方法，分析 Learnable Beta DPO 的优劣势。

## 6. 总结与展望

Learnable Beta DPO 通过引入动态可学习的 $\beta(x; \pi_\theta)$ 函数，有效扩展了标准 DPO 算法。该函数结合可学习参数 $w$、困惑度 $\mathrm{PPL}(x)$ 和神经网络 $f(x)$，使 preference loss 能根据上下文自适应调整。动态 $\beta$ 方法有望提升 DPO 算法的灵活性、适应性和性能，是 DPO 研究的重要方向。未来研究可聚焦于更优 $\beta(x)$ 函数设计、深入理论分析和实验验证，进一步提升 Learnable Beta DPO 的性能和理论完备性。
